# 大規模 PDF 資料提取系統 (P-07, 規劃中)

## 📌 專案簡介
針對極端大規模（3000+ 頁、100MB+）的 PDF 結案報告書，規劃一套自動化數據抽取與清洗流程，將關鍵數值自動填入 Google Sheets。

## 🚀 專案核心目標
- **高性能解析**：解決超大型 PDF 讀取時的記憶體與效能難題。
- **資料清洗 (Data Cleaning)**：類似「門窗表」解析概念，但需應對更多頁數與更複雜的格式。
- **結構化儲存**：將提取後的零散數據轉化為結構化資料庫（Google Sheets/DB）。

## ⚙️ 技術路徑 (探索中)
- **Tool Selection**: 評估 Python PDF 解析庫、GCP Document AI 或其他高性能解析工具。
- **Architectural Design**: 採取「分片處理」或「異步執行」策略以應對極大規模文件。
- **Automation Pipeline**: PDF 上傳 -> 背景解析 -> 自動寫入資料表。

## 🧩 規劃中的亮點
- **大數據思維**：不再只是處理單張圖片，而是考慮如何規模化處理「千頁級」文件的穩定性。
- **問題拆解能力**：將看似不可能的人工閱讀任務，拆解為可工程化的「資料抽取」與「資料清洗」步驟。

---
> **備註**：此項目的規劃展現了開發者在面對超大型數據處理時的技術前瞻性與架構設計能力。
